\documentclass{article}
%\usepackage{fullpage}

\usepackage[english,french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{bbm,algorithmic,algorithm,verbatim}

\usepackage{color}
\usepackage{subcaption}
\usepackage[pdftex]{graphicx}
\usepackage{epsfig}
\usepackage{ulem, stmaryrd, dsfont}

% \usepackage{mathabx} % for \vvvert ||| |||
\usepackage{./tex/sty/scribe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hyperlinks

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\PassOptionsToPackage{hyphens}{url}
\usepackage[pdftex,linkcolor=test,citecolor=vsomb_col,
colorlinks=true,pagebackref,bookmarks=true,plainpages=true,
urlcolor=fb_col]{hyperref}
\usepackage{cleveref}




\begin{document}
\sloppy
\lecture{HMMA307}{REstricted Maximum Likelihood}{Nikolay Oskolkov}{Oph√©lie Coiffier}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}
This project speaks about the REstricted Maximum Likelihood (REML).\\
When we calculate a variance estimator with the Maximum Likelihood, we must check if the estimator isn't biased. Actually, in many cases, it is biased. The obtained value with the Maximum Likelihood method overestimates (or underestimates) the true value. That's why we need to calculate the variance estimator with REML method.\\
In the first part we will illustrate the issue and we will give an answer to the question :
\begin{center}
    How the REML approach affects the linear mixed model ?
\end{center}
Then, we will mathematically explain and solve the problem.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Illustration of our problem}
In the first part, we illustrate the variance problem with an example. 
\begin{figure}[H]
    \begin{center}
    \includegraphics[scale=0.7]{./images/Biased_normal_distri.pdf}
    \caption{Difference between biased Normal distribution and Unbiased Normal distribution.}
    \end{center}
\end{figure}
The Figure 1 shows us the difference between an unbiased Normal distribution and a biased Normal distribution. We see that the mean and the variance are different. On the Figure 1, we use the next data to know the mean of the second Gaussian. It's the estimation of the mean (we will demonstrate it in the next part).\\
Now, we use real data.\\
\begin{table}[h!]
        \centering
        \begin{tabular}{| c | c | c|}
        \hline
        \begin{bf} Ind \end{bf} &
        \begin{bf} Resp \end{bf} &
        \begin{bf} Treat \end{bf} \\
        \hline
        1 &  10 & 0\\
        1 & 25 & 1 \\
        2 & 3 & 0 \\
        2 &  6 & 1\\
        \hline
        \end{tabular}
\end{table}
\textit{Ind} column is the group of the individual, \textit{Resp} column is the treatment response and \textit{Treat} column is an indicator (if the individual gets the treatment, he has 1 else he has 0).\\
The model used in linear regression is the impact of the treatment on the response.
\begin{equation*}
    Y_{Resp} = \mu +  \beta X_{Treat} + \varepsilon
\end{equation*}
Where $\varepsilon$ is a noise following a centered, reduced Normal distribution ($\mathcal{N}(0, 1)$).\\
The model used in linear mixed effects is :
\begin{equation*}
    Y_{Resp} = \mu + \beta X_{Treat}  + \alpha X_{Ind} +\varepsilon
\end{equation*}
When we compare the log-likelihood with both methods, we don't obtain the same result.
With the linear regression, we have $-14.23$ (we can observe this in the Figure 2, at the point $(8, -14.23)$) while we find $-7.89$ when we use the linear mixed effects regression (REML). The REML method has an higher log-likelihood than the maximum likelihood method. It's normal because the REML method has an additional term in its log-likelihood (calculations are demonstrated in next parts).  \\
\begin{remark}
\textit{The code is available in the repository called REML but we notice lines we use to show the log-likelihood comparison}
\end{remark}
\begin{lstlisting}
  linear_reg = sm.OLS(df.Resp, df.Treat)
  linear_reg_fit = linear_reg.fit()
  linear_reg_fit.summary()
\end{lstlisting}
\begin{lstlisting}
  mixed_random8REML = smf.mixedlm("Resp~Treat", df, groups = df['Ind'])
  mixed_fit_REML = mixed_random_REML.fit()
  print(mixed_fit_REML.summary()) 
\end{lstlisting}
\begin{lstlisting}
  mixed_random_ML = smf.mixedlm("Resp ~ Treat", df, groups = df['Ind'])
  mixed_fit_ML = mixed_random_ML.fit(reml=False)
  print(mixed_fit_ML.summary()) 
\end{lstlisting}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.7\textwidth}
        \includegraphics[width=\textwidth]{./images/Log_likelihood.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=\textwidth]{./images/OLS_Regression.pdf}
    \end{subfigure}
    \caption{The log-likelihood using linear regression depending on $\sigma$ and the result of the linear regression with Python function. }
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.3]{./images/REML_regression.pdf}
        \caption{Values of the linear mixed model using REML method.}
    \end{center}
\end{figure}
Moreover, we notice the different value of the coefficient : $\sigma^2 =\sqrt{scale}= \sqrt{36.00}= 6.00$\\
$\sigma_s^2 = \sqrt{Group Var}=\sqrt{66.5} = 8.15$, $\beta_1 = 6.5$ and $\beta_2 = (6.5+9.0) = 15.5$\\
\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.4]{./images/OLS_regression2.pdf}
        \caption{Values of the linear model using least squared method.}
    \end{center}
\end{figure}
Now, we admit the next results and we demonstrate it in the other parts of this report.\\
Thanks to that values, we will compare our results with Python function results.\\
We write out :
\[Y = \begin{pmatrix} 3 & 10 \\ 6 & 25 \end{pmatrix}\]
\[X = \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 0 \\ 0 & 1 \end{pmatrix}\]
\[ \Sigma_y = \begin{pmatrix} \sigma^2+\sigma_s^2 & \sigma_s^2 & 0 & 0 \\ \sigma_s^2 & \sigma_s^2+\sigma^2 & 0 & 0 \\ 0 & 0 & \sigma^2+\sigma_s^2 & \sigma_s^2 \\ 0 & 0 & \sigma_s^2 & \sigma_s^2+\sigma^2 \end{pmatrix}\]
\[|\Sigma_y| = 4\sigma_s^4\sigma^4+4\sigma_s^2\sigma^6+\sigma^8\]
\\
Where, $\Sigma_y$ is the variance-covariance matrix, $|\Sigma_y|$ its determinant.\\
So, we can maximize the integrated log-likelihood (REML purpose, detailed in next part).\\
\textit{The code is available in the Github field (REML.py)}.\\
Actually, we define a function calculating the integrated log-likelihood with two parameters that aren't fixed : $\sigma^2$ and $\sigma_s^2$. We know $\beta$ thanks to the linear regression and Y is represented by the \textit{Resp} values. The implemented function is :
\[ log(\int L(\beta, \Sigma_y)d\beta)=-\frac{1}{2}log(2\pi)-\frac{1}{2}log(|\Sigma_y|)-\frac{\trans{(Y-X\beta)}\Sigma_y^{-1}(Y-X\beta)}{2}-\frac{1}{2}log(|\trans{X}\Sigma_y^{-1}X|)\] 
(This equality will be explained in the next part). Then, we search the maximum of this function. Our results are the value of $\sigma^2$ and $\sigma_s^2$ which maximize the function.\\
We obtain : $log(\int L(\beta, \Sigma_y)d\beta)=-6.05$, $\sigma_s^2 = 8.15$ and $\sigma^2=6.00$. \\
These are the same results that the values calculating with linear regression.\\
Now, we compare these results with the linear mixed effects results using the maximum likelihood. We will see the impact of the REML method on linear mixed model. 
\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.3]{./images/ML_mixed_effects.pdf}
        \caption{Values of the linear mixed model using Maximum Likelihood method.}
    \end{center}
\end{figure}
We have the same results for fixed coefficients but standard deviation for random effects and residual standard deviation are different : $\sigma^2 = \sqrt{18.0} = 4.24$ and $\sigma_s^2 = \sqrt{33.25}=5.77$.\\
Finally, we find some differences. The REML method changes the linear mixed model by the standard deviation and (obviously) the log-likelihood. But it doesn't change the fixed coefficient. We observe that the maximum likelihood method underestimates the true variance. But we must demonstrate it, to be sure.\\
\newline
In the next part, we will explain the issue and solve it with REML method.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The biased variance problem}
When we calculate the maximum likelihood, we transform the likelihood to log-likelihood, then we derive and we equate to zero. And, obviously, we calculate the second derivative of the log-likelihood to check the sign. We need a negative sign to have a maximum.\\
\begin{example}
First, we calculate the maximum likelihood in $1$ dimension.\\
We take
\[ y = (y_1, \cdots, y_N) \sim \mathcal{N}(\mu, \sigma^2) \]
where $\mu$ is the mean and $\sigma^2$ is the variance.\\
The likelihood is \[ L(y, \mu, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt(2\pi \sigma^2)}e^{-\frac{(y_i-\mu)^2}{2\sigma^2}} \]
The log-likelihood is \[l(y, \mu, \sigma^2) = -\frac{N}{2}ln(2\pi\sigma^2)-\sum_{i=1}^N\frac{(y_i-\mu)^2}{2\sigma^2}\]
Now, we derive this function and equate to zero to find the maximum
\begin{equation*}
    \begin{cases}
        \frac{\partial}{\partial \mu}\mid_{\hat{\mu}}l(y, \mu, \sigma^2) = 0 \\
        \frac{\partial}{\partial \sigma^2})\mid_{\widehat{\sigma^2}}l(y, \mu, \sigma^2) = 0
    \end{cases}
\end{equation*}
We assure that we have a negative second derivative.\\
Finally, we find \[ (\hat{\mu}, \widehat{\sigma^2}) = (\frac{1}{N} \sum_{i=1}^N y_i, \frac{1}{N} \sum_{i=1}^N(y_i-\hat{\mu})^2)\]
\end{example}
Usually, we stop there, we have our answer but if we calculate the expected value of the variance estimator, we should obtain the variance estimator (if it's an unbiased estimator).\\
\begin{example}
Return to the previous example. We need to know if the variance estimator is unbiased.\\
We write out $\hat{y} = \frac{1}{N} \sum_{i=1}^N y_i = \hat{\mu}$
\begin{equation*}
\begin{split}
   \esp[\widehat{\sigma^2}] = \esp[\frac{1}{N} \sum_{i=1}^N(y_i-\hat{\mu})^2] 
   &= \esp[\frac{1}{N} \sum_{i=1}^N(y_i-\mu + \mu - \hat{\mu})^2]\\
   &= \esp[\frac{1}{N} \sum_{i=1}^N((y_i-\mu)^2 + (\hat{\mu}-\mu)^2 - 2(y_i-\mu)(\hat{\mu}-\mu))]
   \end{split}
\end{equation*}
But, we have $\hat{\mu}-\mu = \frac{1}{N}\sum_{i=1}^N y_i-\mu = \frac{1}{N}\sum_{i=1}^N(y_i-\mu)$ \\
So, $\sum_{i=1}^N(y_i-\mu) = N(\hat{\mu}-\mu)$\\
Let's return to our expected value
\begin{equation*}
    \begin{split}
        \esp[\widehat{\sigma^2}] = \frac{1}{N} \sum_{i=1}^N \esp[(y_i-\mu)^2]-\frac{2}{N}\esp[N(\hat{\mu}-\mu)^2]+\esp[(\hat{\mu}-\mu)^2] 
        &=  \frac{1}{N} \sum_{i=1}^N\esp[(y_i-\mu)^2]-\esp[(\hat{\mu}-\mu)^2]
    \end{split}
\end{equation*}
We need to find the variance of $(y_i-\mu)$ to carry on our calculation.
\[\\Var(y_i-\mu) = \esp[(y_i-\mu)^2]-(\esp[(y_i-\mu)])^2 = \esp[(y_i-\mu)^2] = \sigma^2\]
Finally, we have these equations
\begin{equation*}
    \esp[\widehat{\sigma^2}] = \sigma^2 - \esp[(\hat{\mu}-\mu)^2] = \sigma^2 - \Var(\hat{\mu}-\mu) = \sigma^2 - \frac{1}{N^2}\Var(\sum_{i=1}^N y_i) = \sigma^2\frac{N-1}{N} \ne \sigma^2
\end{equation*}
The variance estimator is biased (underestimated the true variance because $\frac{N-1}{N}<1$).To remove the bias, we can change the variance estimator : $\widehat{\sigma^2} = \frac{1}{N-1}\sum_{i=1}^N (y_i-\hat{\mu})^2$.
\end{example}

We also put these calculations in a higher dimension. In the real life, e.g our illustration, the dimension isn't $1$ but $k$ where $k>1$.\\
The model writes out $Y=X \beta + \varepsilon$.
where $\varepsilon$ follows a Normal distribution $\mathcal{N}(0, \sigma^2 I_k)$ and $Y$ follows a Normal distribution $\mathcal{N}(X\beta, \sigma^2I_k)$.\\
The log-likelihood becomes 
\[ l(\beta, \sigma_2) = -\frac{N}{2}log(2\pi)-\frac{N}{2} log(\sigma^2)-\frac{1}{2\sigma^2}\trans{(Y-X\beta)}(Y-X\beta) \]
\\
We don't realize all calculations but it the same principle than with $1$ dimension. So, we obtain these estimators \\
\[ \hat{\beta} = (\trans{X}{X})^{-1}\trans{X}Y \] and \[ \widehat{\sigma^2} = \frac{1}{N}\trans{(Y-X\hat{\beta})}(Y-X\hat{\beta})\]
We calculate the expected value of the variance estimator.
\begin{equation*}
    \begin{split}
       \esp[\widehat{\sigma^2}]=\frac{1}{N}\esp[\trans{(Y-X\hat{\beta})}(Y-X\hat{\beta}]
       &\textcolor{blue}{=} \frac{1}{N}\esp[\trans{Y}(I_k-A)(I_k-A)Y]\\
       &= \frac{1}{N}\esp[\trans{Y}(I_k-A)Y]\\
       &= \frac{1}{N}\esp[\trans{Y}Y-\trans{Y}AY]\\
       &= \frac{1}{N}(\esp[\trans{Y}Y]-\esp[\trans{Y}AY])\\
       &= \frac{1}{N}(N\sigma^2+\trans{(X\beta)}(X\beta)-(k\sigma^2+\trans{(X\beta)}(X\beta))\\
       &= \frac{N-k}{N}\sigma^2
    \end{split}
\end{equation*}
where $k=rg(A)$ is the number of $X$'s column.\\
\textcolor{blue}{We notice $A=X(\trans{X}X)^{-1}\trans{X}$ and consequently, $AY=X\hat{\beta} = \hat{Y}$}\\
\\
Finally, we have a bias, but if we choose $\widehat{\sigma^2}=\frac{1}{N-k}\trans{(Y-X\hat{\beta})}(Y-X\hat{\beta})$, we don't have a bias.\\
\\
Thanks to these calculations, we see that the maximum likelihood is a good method when $k << N$. But the biased results are obtained when $N\approx k$.\\
\\
That's why, we use the REML method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solve the bias problem}
The main issue is that we use an unknown estimator for the mean. The principle that we will use is : if the log-likelihood has any information about the mean, we can optimize it and find a unbiased variance estimator.\\
\begin{itemize}
    \item First step : integrate the likelihood in relation to $\mu$ and calculate the log of this integration. The parameter $\mu$ will be removed from the equation.
    \item Second step : use Taylor development in the log-likelihood to simplify the formula. 
    \item Third step : separate the formula in $2$ parts (the log-likelihood use in the maximum likelihood and the bias, the REML approach).
    \item Fourth step : finish previous calculations to find the unbiased estimator.
\end{itemize}

\begin{example} 
We continue with the previous example (the Normal distribution and $\beta \in \bbR^{2\times 2}$). The likelihood is
\[L(\beta, \sigma_s^2, \sigma^2) = \frac{1}{\sqrt{2\pi |\Sigma_y|}}e^{-\frac{\trans{(Y-X\beta)}\Sigma_y^{-1}(Y-X\beta)}{2}}\]
where $\sigma_s^2$ represents standard deviation of random effects, $\sigma^2$  represents standard deviance of residual effects and $|\Sigma_y|$ is the variance-covariance matrix.\\
First step :
\[log(\int L(\beta, \Sigma_y)d\beta) = -\frac{1}{2}log(2\pi)-\frac{1}{2}log(|\Sigma_y|)+log(\int e^{-\frac{\trans{(Y-X\beta)}\Sigma_y^{-1}(Y-X\beta)}{2}} d\beta)\]
We write out $f(\beta) = -\frac{\trans{(Y-X\beta)}\Sigma_y^{-1}(Y-X\beta)}{2}$ and we use the Taylor development formula : \\
$f(\beta) \approx f(\hat{\beta}) + \frac{1}{2}(\beta - \hat{\beta})^2f''(\hat{\beta})$\\
Therefore, we obtain :
\[f(\beta) \approx -\frac{\trans{(Y-X\beta)}\Sigma_y^{-1}(Y-X\beta)}{2}-\frac{1}{2}\frac{\trans{(\beta-\hat{\beta})} \trans{X} \Sigma_y^{-1}X (\beta-\hat{\beta})}{2}\]

So, the new log-likelihood is :
\begin{equation*}
    \begin{split}
       log(\int L(\beta, \Sigma_y)d\beta) = -\frac{1}{2}log(2\pi)-\frac{1}{2}log(|\Sigma_y|)+log(\int e^{-\frac{1}{2}\frac{\trans{(\beta-\hat{\beta})} \trans{X} \Sigma_y^{-1}X (\beta-\hat{\beta})}{2} d\beta}) -\frac{\trans{(Y-X\beta)}\Sigma_y^{-1}(Y-X\beta)}{2}\\
       =-\frac{1}{2}log(2\pi)-\frac{1}{2}log(|\Sigma_y|)-\frac{\trans{(Y-X\beta)}\Sigma_y^{-1}(Y-X\beta)}{2} - \frac{1}{2}log(|\trans{X}\Sigma_y^{-1}X|)  
    \end{split}
\end{equation*}
We recognize the solution of the Maximum Likelihood
\[-\frac{1}{2}log(2\pi)-\frac{1}{2}log(|\Sigma_y|)-\frac{\trans{(Y-X\beta)}\Sigma_y^{-1}(Y-X\beta)}{2}\]
And we have the REML approach (the bias) : \[- \frac{1}{2}log(|\trans{X} \Sigma_y^{-1}X|\]
\end{example}
To conclude calculations, we derivate the new log-likelihood in relation to $\sigma^2$ and equate to $0$. The solution is the unbiased estimator of the variance. There are the same calculations that we realise to find the maximum likelihood.\\
\\
In the first part of the document, we have seen an applied example. It shows us how the fee affects the mixed linear model.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deal with linear models in depth}
\label{sec:pour_aller_plus_loin_sur_ce_theme}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can read the article and the lesson to learn more about the REML and the linear regression model:
\cite{Google}, \cite{Teaching}\\
To visualize the Python code, we can go to the Github :
\cite{Code}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{alpha}
\bibliography{./tex/biblio/references_all}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}